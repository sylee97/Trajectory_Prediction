{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f05529c-6672-4a53-8a98-ab138b55efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\NGN\\\\dev\\\\Traffino\\\\TRAFFINO\\\\traffino\\\\data')\n",
    "from trajectories import TrajectoryDataset, seq_collate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14de37f-3622-4a93-a6a1-86aca0a033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95f2e2f-bd38-4eb3-85b4-611b637b378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\NGN\\\\dev\\\\Traffino\\\\TRAFFINO\\\\traffino\\\\datasets'\n",
    "obs_len = 8\n",
    "pred_len = 8\n",
    "skip = 1\n",
    "delim = '\\t'\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72603cae-d2f6-4bfc-b3cc-22c8140c28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    dset = TrajectoryDataset(\n",
    "        path,\n",
    "        obs_len,# args.obs_len,\n",
    "        pred_len, # args.pred_len,\n",
    "        skip,# args.skip,\n",
    "        delim# args.delim\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dset,\n",
    "        batch_size, # args.batch_size,\n",
    "        shuffle,\n",
    "        num_workers, # args.loader_num_workers,\n",
    "        collate_fn=seq_collate)\n",
    "    return dset, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb52922-d516-4385-9001-0ff8063d97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'C:\\\\Users\\\\NGN\\\\dev\\\\Traffino\\\\TRAFFINO\\\\traffino\\\\datasets\\\\0769_csv2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d395a67-1207-4759-b700-4568050ae79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "    def __init__(\n",
    "        self, data_dir, obs_len=8, pred_len=12, skip=1, \n",
    "        # threshold=0.002, min_ped=1, \n",
    "        delim='\\t'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format\n",
    "        <frame_id> <agent_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non linear traj\n",
    "        when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a seqeunce\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        # self.data_dir = data_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        print(all_files)\n",
    "        all_files = [os.path.join(self.data_dir, _path) for _path in all_files]\n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        seq_list_rel = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        for path in all_files:\n",
    "            data = read_file(path, delim)\n",
    "            frames = np.unique(data[:, 0]).tolist()\n",
    "            frame_data = []\n",
    "            for frame in frames:\n",
    "                frame_data.append(data[frame == data[:, 0], :])\n",
    "            num_sequences = int(\n",
    "                math.ceil((len(frames) - self.seq_len + 1) / skip))\n",
    "\n",
    "            for idx in range(0, num_sequences * self.skip + 1, skip):\n",
    "                curr_seq_data = np.concatenate(\n",
    "                    frame_data[idx:idx + self.seq_len], axis=0)\n",
    "                peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "                curr_seq_rel = np.zeros((len(peds_in_curr_seq), 2,\n",
    "                                         self.seq_len))\n",
    "                curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n",
    "                curr_loss_mask = np.zeros((len(peds_in_curr_seq),\n",
    "                                           self.seq_len))\n",
    "                num_peds_considered = 0\n",
    "                _non_linear_ped = []\n",
    "                for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "                    curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] ==\n",
    "                                                 ped_id, :]\n",
    "                    curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                    pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                    pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                    if pad_end - pad_front != self.seq_len:\n",
    "                        continue\n",
    "                    curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                    curr_ped_seq = curr_ped_seq\n",
    "                    # Make coordinates relative\n",
    "                    rel_curr_ped_seq = np.zeros(curr_ped_seq.shape)\n",
    "                    rel_curr_ped_seq[:, 1:] = \\\n",
    "                        curr_ped_seq[:, 1:] - curr_ped_seq[:, :-1]\n",
    "                    _idx = num_peds_considered\n",
    "                    curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "                    curr_seq_rel[_idx, :, pad_front:pad_end] = rel_curr_ped_seq\n",
    "                    # Linear vs Non-Linear Trajectory\n",
    "                    _non_linear_ped.append(\n",
    "                        poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                    curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                    num_peds_considered += 1\n",
    "\n",
    "                if num_peds_considered > min_ped:\n",
    "                    non_linear_ped += _non_linear_ped\n",
    "                    num_peds_in_seq.append(num_peds_considered)\n",
    "                    loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                    seq_list.append(curr_seq[:num_peds_considered])\n",
    "                    seq_list_rel.append(curr_seq_rel[:num_peds_considered])\n",
    "\n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        seq_list_rel = np.concatenate(seq_list_rel, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.obs_traj = torch.from_numpy(\n",
    "            seq_list[:, :, :self.obs_len]).type(torch.float)\n",
    "        self.pred_traj = torch.from_numpy(\n",
    "            seq_list[:, :, self.obs_len:]).type(torch.float)\n",
    "        self.obs_traj_rel = torch.from_numpy(\n",
    "            seq_list_rel[:, :, :self.obs_len]).type(torch.float)\n",
    "        self.pred_traj_rel = torch.from_numpy(\n",
    "            seq_list_rel[:, :, self.obs_len:]).type(torch.float)\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [\n",
    "            (start, end)\n",
    "            for start, end in zip(cum_start_idx, cum_start_idx[1:])\n",
    "        ]\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [\n",
    "            self.obs_traj[start:end, :], self.pred_traj[start:end, :],\n",
    "            self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\n",
    "            self.non_linear_ped[start:end], self.loss_mask[start:end, :]\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176391e6-8ff0-48b1-bd83-08cafbae2a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dset, train_loader \u001b[39m=\u001b[39m data_loader()\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_loader\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     dset \u001b[39m=\u001b[39m TrajectoryDataset(\n\u001b[0;32m      3\u001b[0m         path,\n\u001b[0;32m      4\u001b[0m         obs_len,\u001b[39m# args.obs_len,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m         pred_len, \u001b[39m# args.pred_len,\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m         skip,\u001b[39m# args.skip,\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m         delim\u001b[39m# args.delim\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m     11\u001b[0m         dset,\n\u001b[0;32m     12\u001b[0m         batch_size, \u001b[39m# args.batch_size,\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         shuffle,\n\u001b[0;32m     14\u001b[0m         num_workers, \u001b[39m# args.loader_num_workers,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         collate_fn\u001b[39m=\u001b[39mseq_collate)\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m dset, loader\n",
      "File \u001b[1;32mc:\\Users\\NGN\\dev\\Traffino\\TRAFFINO\\traffino\\data\\trajectories.py:162\u001b[0m, in \u001b[0;36mTrajectoryDataset.__init__\u001b[1;34m(self, data_dir, obs_len, pred_len, skip, delim)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[39m# if num_peds_considered > min_ped:\u001b[39;00m\n\u001b[0;32m    155\u001b[0m         \u001b[39m#     non_linear_ped += _non_linear_ped\u001b[39;00m\n\u001b[0;32m    156\u001b[0m         \u001b[39m#     num_peds_in_seq.append(num_peds_considered)\u001b[39;00m\n\u001b[0;32m    157\u001b[0m         \u001b[39m#     loss_mask_list.append(curr_loss_mask[:num_peds_considered])\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         \u001b[39m#     seq_list.append(curr_seq[:num_peds_considered])\u001b[39;00m\n\u001b[0;32m    159\u001b[0m         \u001b[39m#     seq_list_rel.append(curr_seq_rel[:num_peds_considered])\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_seq \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(seq_list)\n\u001b[1;32m--> 162\u001b[0m seq_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(seq_list, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m seq_list_rel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(seq_list_rel, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    164\u001b[0m loss_mask_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(loss_mask_list, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "train_dset, train_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031c0c6a-f9c7-4766-8e20-71cb93b368c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 5.38830830e+05 4.81401168e+06]\n",
      " [1.00000000e+00 2.00000000e+00 5.38801690e+05 4.81398491e+06]\n",
      " [1.00000000e+00 3.00000000e+00 5.38840210e+05 4.81399683e+06]\n",
      " ...\n",
      " [8.72000000e+03 2.80000000e+02 5.38846450e+05 4.81398814e+06]\n",
      " [8.72000000e+03 2.82000000e+02 5.38832150e+05 4.81402206e+06]\n",
      " [8.72000000e+03 2.83000000e+02 5.38838200e+05 4.81401869e+06]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "data = []\n",
    "with open(train_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(delim)\n",
    "        line = [float(i) for i in line]\n",
    "        data.append(line)\n",
    "print(np.asarray(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aae38-855c-4d11-ab85-f4754eb9e6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waterloo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
