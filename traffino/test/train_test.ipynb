{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\NGN\\\\dev\\\\Traffino\\\\TRAFFINO')\n",
    "import argparse\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from traffino.data.loader import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from traffino.losses import gan_g_loss, gan_d_loss, l2_loss\n",
    "# from traffino.losses import displacement_error, final_displacement_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffino.models import TrajectoryGenerator, TrajectoryDiscriminator\n",
    "from traffino.utils import int_tuple, bool_flag, get_total_norm\n",
    "from traffino.utils import relative_to_abs, get_dset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True # cudnn의 benchmark를 통해 최적 backend 연산을 찾는 flag를 true로 하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "# # Dataset options\n",
    "# parser.add_argument('--dataset_name', default='zara1', type=str)\n",
    "# parser.add_argument('--delim', default=' ')\n",
    "# parser.add_argument('--loader_num_workers', default=4, type=int)\n",
    "# parser.add_argument('--obs_len', default=8, type=int)\n",
    "# parser.add_argument('--pred_len', default=8, type=int)\n",
    "# parser.add_argument('--skip', default=1, type=int)\n",
    "\n",
    "# # Optimization\n",
    "# parser.add_argument('--batch_size', default=64, type=int)\n",
    "# parser.add_argument('--num_iterations', default=10000, type=int)\n",
    "# parser.add_argument('--num_epochs', default=200, type=int)\n",
    "\n",
    "# # Model Options\n",
    "# parser.add_argument('--embedding_dim', default=64, type=int)\n",
    "# parser.add_argument('--num_layers', default=1, type=int)\n",
    "# parser.add_argument('--dropout', default=0, type=float)\n",
    "# parser.add_argument('--batch_norm', default=0, type=bool_flag)\n",
    "# parser.add_argument('--mlp_dim', default=1024, type=int)\n",
    "\n",
    "# # Generator Options\n",
    "# parser.add_argument('--encoder_h_dim_g', default=64, type=int)\n",
    "# parser.add_argument('--decoder_h_dim_g', default=128, type=int)\n",
    "# parser.add_argument('--noise_dim', default=None, type=int_tuple)\n",
    "# parser.add_argument('--noise_type', default='gaussian')\n",
    "# parser.add_argument('--noise_mix_type', default='ped')\n",
    "# parser.add_argument('--clipping_threshold_g', default=0, type=float)\n",
    "# parser.add_argument('--g_learning_rate', default=5e-4, type=float)\n",
    "# parser.add_argument('--g_steps', default=1, type=int)\n",
    "\n",
    "# # Pooling Options\n",
    "# parser.add_argument('--pooling_type', default='pool_net')\n",
    "# parser.add_argument('--pool_every_timestep', default=1, type=bool_flag)\n",
    "\n",
    "# # Pool Net Option\n",
    "# parser.add_argument('--bottleneck_dim', default=1024, type=int)\n",
    "\n",
    "# # Social Pooling Options\n",
    "# parser.add_argument('--neighborhood_size', default=2.0, type=float)\n",
    "# parser.add_argument('--grid_size', default=8, type=int)\n",
    "\n",
    "# # Discriminator Options\n",
    "# parser.add_argument('--d_type', default='local', type=str)\n",
    "# parser.add_argument('--encoder_h_dim_d', default=64, type=int)\n",
    "# parser.add_argument('--d_learning_rate', default=5e-4, type=float)\n",
    "# parser.add_argument('--d_steps', default=2, type=int)\n",
    "# parser.add_argument('--clipping_threshold_d', default=0, type=float)\n",
    "\n",
    "# # Loss Options\n",
    "# parser.add_argument('--l2_loss_weight', default=0, type=float)\n",
    "# parser.add_argument('--best_k', default=1, type=int)\n",
    "\n",
    "# # Output\n",
    "# parser.add_argument('--output_dir', default=os.getcwd())\n",
    "# parser.add_argument('--print_every', default=5, type=int)\n",
    "# parser.add_argument('--checkpoint_every', default=100, type=int)\n",
    "# parser.add_argument('--checkpoint_name', default='checkpoint')\n",
    "# parser.add_argument('--checkpoint_start_from', default=None)\n",
    "# parser.add_argument('--restore_from_checkpoint', default=1, type=int)\n",
    "# parser.add_argument('--num_samples_check', default=5000, type=int)\n",
    "\n",
    "# # Misc\n",
    "# parser.add_argument('--use_gpu', default=1, type=int)\n",
    "# parser.add_argument('--timing', default=0, type=int)\n",
    "# parser.add_argument('--gpu_num', default=\"0\", type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class CreateArg():\n",
    "    def __init__(self):\n",
    "        # Dataset options\n",
    "        self.dataset_name = 'waterloo'\n",
    "        self.delim = '\\t'\n",
    "        self.loader_num_workers = 4\n",
    "        self.obs_len = 8\n",
    "        self.pred_len = 8\n",
    "        self.skip = 1\n",
    "        # Optimization\n",
    "        self.batch_size = 64\n",
    "        self.num_iterations = 10000\n",
    "        self.num_epochs = 200                      \n",
    "        # Model Options\n",
    "        self.embedding_dim = 64\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0.0\n",
    "        self.batch_norm = 0 \n",
    "        self.mlp_dim = 1024\n",
    "                                    \n",
    "        # Generator Options\n",
    "        self.encoder_h_dim_g = 64\n",
    "        self.decoder_h_dim_g = 128\n",
    "        self.noise_dim : Tuple[int] = (0, 0) # default=None # type=int_tuple\n",
    "        self.noise_type = 'gaussian'\n",
    "        self.noise_mix_type = 'ped'\n",
    "        self.clipping_threshold_g = 0 # type=float\n",
    "        self.g_learning_rate = 5e-4 # type=float \n",
    "        self.g_steps = 1\n",
    "\n",
    "        # Pooling Options\n",
    "        self.pooling_type = 'pool_net' \n",
    "        self.pool_every_timestep = 1 # type=bool_flag\n",
    "\n",
    "        # Pool Net Option\n",
    "        self.bottleneck_dim = 1024 # type=int\n",
    "\n",
    "        # Social Pooling Options\n",
    "        self.neighborhood_size = 1024 # type=float\n",
    "        self.grid_size = 8 # type=int\n",
    "\n",
    "        # Discriminator Options\n",
    "        self.d_type = 'local' # type=str\n",
    "        self.encoder_h_dim_d = 64 # type=int\n",
    "        self.d_learning_rate = 5e-4 # type=float\n",
    "        self.d_steps = 2 # type=int        \n",
    "        self.clipping_threshold_d = 0 # type=float  \n",
    "\n",
    "        # Loss Options\n",
    "        self.l2_loss_weight = 0 # type=float \n",
    "        self.best_k = 1 # type=int \n",
    "\n",
    "        # Output\n",
    "        self.output_dir = os.getcwd()\n",
    "        self.print_every = 5 # type=int\n",
    "        self.checkpoint_every = 100 # type=int\n",
    "        self.checkpoint_name = 'checkpoint' \n",
    "        self.checkpoint_start_from = None\n",
    "        self.restore_from_checkpoint = 1 # type=int\n",
    "        self.num_samples_check = 5000 # type=int        \n",
    "\n",
    "        # Misc\n",
    "        self.use_gpu = 1 # type=int\n",
    "        self.timing = 0 # type=int\n",
    "        self.gpu_num = \"0\" # type=str   \n",
    "\n",
    "\n",
    "args = CreateArg() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(args.use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "def get_dtypes(args):\n",
    "    long_dtype = torch.LongTensor\n",
    "    float_dtype = torch.FloatTensor\n",
    "    if args.use_gpu == 1:\n",
    "    # use_gpu == 1:\n",
    "        long_dtype = torch.cuda.LongTensor\n",
    "        float_dtype = torch.cuda.FloatTensor\n",
    "    return long_dtype, float_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_dtype, float_dtype = get_dtypes(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\waterloo\\val\n"
     ]
    }
   ],
   "source": [
    "# train_path = get_dset_path(args.dataset_name, 'train') # dset_name, dset_type\n",
    "# print(train_path) # datasets\\waterloo\\train\n",
    "train_path = 'C:\\\\Users\\\\NGN\\\\dev\\\\Traffino\\\\TRAFFINO\\\\traffino\\\\datasets\\\\waterloo\\\\train'\n",
    "val_path = get_dset_path(args.dataset_name, 'val')\n",
    "print(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO: 1364202733.py:    1]: Initializing train dataset\n",
      "['0769_2.txt']\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initializing train dataset\")\n",
    "train_dset, train_loader = data_loader(args, train_path)\n",
    "# logger.info(\"Initializing val dataset\")\n",
    "# _, val_loader = data_loader(args, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO: 2378675245.py:    5]: There are 62.3359375 iterations per epoch\n"
     ]
    }
   ],
   "source": [
    "iterations_per_epoch = len(train_dset) / args.batch_size / args.d_steps\n",
    "if args.num_epochs:\n",
    "    args.num_iterations = int(iterations_per_epoch * args.num_epochs)\n",
    "\n",
    "logger.info(\n",
    "    'There are {} iterations per epoch'.format(iterations_per_epoch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TrajectoryGenerator(\n",
    "    obs_len=args.obs_len,\n",
    "    pred_len=args.pred_len,\n",
    "    embedding_dim=args.embedding_dim,\n",
    "    encoder_h_dim=args.encoder_h_dim_g,\n",
    "    decoder_h_dim=args.decoder_h_dim_g,\n",
    "    mlp_dim=args.mlp_dim,\n",
    "    num_layers=args.num_layers,\n",
    "    noise_dim=args.noise_dim,\n",
    "    noise_type=args.noise_type,\n",
    "    noise_mix_type=args.noise_mix_type,\n",
    "    pooling_type=args.pooling_type,\n",
    "    pool_every_timestep=args.pool_every_timestep,\n",
    "    dropout=args.dropout,\n",
    "    bottleneck_dim=args.bottleneck_dim,\n",
    "    neighborhood_size=args.neighborhood_size,\n",
    "    grid_size=args.grid_size,\n",
    "    batch_norm=args.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO: 1295372059.py:    3]: Here is the generator:\n",
      "[INFO: 1295372059.py:    4]: TrajectoryGenerator(\n",
      "  (encoder): TrajEncoder(\n",
      "    (encoder): LSTM(64, 64)\n",
      "    (spatial_embedding): Linear(in_features=2, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): LSTM(64, 128)\n",
      "    (spatial_embedding): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (hidden2pos): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (pool_net): PoolHiddenNet(\n",
      "      (spatial_embedding): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (mlp_pre_pool): Sequential(\n",
      "        (0): Linear(in_features=192, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1152, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (pool_net): PoolHiddenNet(\n",
      "    (spatial_embedding): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (mlp_pre_pool): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (mlp_decoder_context): Sequential(\n",
      "    (0): Linear(in_features=1088, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator.apply(init_weights)\n",
    "generator.type(float_dtype).train()\n",
    "logger.info('Here is the generator:')\n",
    "logger.info(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = TrajectoryDiscriminator(\n",
    "    obs_len=args.obs_len,\n",
    "    pred_len=args.pred_len,\n",
    "    embedding_dim=args.embedding_dim,\n",
    "    h_dim=args.encoder_h_dim_d,\n",
    "    mlp_dim=args.mlp_dim,\n",
    "    num_layers=args.num_layers,\n",
    "    dropout=args.dropout,\n",
    "    batch_norm=args.batch_norm,\n",
    "    d_type=args.d_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO: 2576673517.py:    3]: Here is the discriminator:\n",
      "[INFO: 2576673517.py:    4]: TrajectoryDiscriminator(\n",
      "  (encoder): TrajEncoder(\n",
      "    (encoder): LSTM(64, 64)\n",
      "    (spatial_embedding): Linear(in_features=2, out_features=64, bias=True)\n",
      "  )\n",
      "  (real_classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "discriminator.apply(init_weights)\n",
    "discriminator.type(float_dtype).train()\n",
    "logger.info('Here is the discriminator:')\n",
    "logger.info(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waterloo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
